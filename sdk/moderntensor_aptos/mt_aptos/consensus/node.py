# sdk/consensus/node.py
"""
Defines the ValidatorNode class, containing the main logic for coordinating
the consensus cycle. Uses asyncio for network tasks and waiting.
Relies on centralized settings from mt_aptos.config.settings.

*** This is a detailed framework; specific logic needs completion/overrides. ***
"""
import os
import random
import time
import json
import math
import asyncio
import httpx
import sys
from typing import List, Dict, Any, Tuple, Optional, Set
from collections import defaultdict, OrderedDict
import logging
import string
import psutil
import uvicorn
from ..monitoring.health import app as health_app
from ..monitoring.circuit_breaker import CircuitBreaker
from ..monitoring.rate_limiter import RateLimiter

# --- Import Settings ---
from mt_aptos.config.settings import settings

# --- Import các module khác trong SDK ---
# Formulas
from mt_aptos.formulas import *  # Import tất cả hoặc import cụ thể

# Metagraph & Blockchain Interaction
from mt_aptos.core.datatypes import CycleConsensusResults, MinerConsensusResult
from mt_aptos.metagraph.metagraph_data import get_all_miner_data, get_all_validator_data
from mt_aptos.metagraph import metagraph_data
from mt_aptos.metagraph.metagraph_datum import (
    MinerData,
    ValidatorData,
    STATUS_ACTIVE,
    STATUS_JAILED,
    STATUS_INACTIVE,
)
from mt_aptos.metagraph.hash.hash_datum import hash_data  # Import hàm hash thật sự
from mt_aptos.async_client import RestClient
from mt_aptos.account import Account

# Aptos imports
from mt_aptos.aptos_core.contract_client import AptosContractClient, create_aptos_client
from mt_aptos.aptos_core.context import get_aptos_context
from mt_aptos.aptos_core.account_service import check_account_exists, get_account_balance
from mt_aptos.aptos_core.validator_helper import get_validator_info, get_all_validators, get_all_miners

# Mock function for history decoding (to be implemented later)
async def decode_history_from_hash(hash_str):
    await asyncio.sleep(0)
    return []  # Mock decode


# Network Models (for task/result data structure)
from mt_aptos.network.server import TaskModel, ResultModel

# Core Datatypes
from mt_aptos.core.datatypes import (
    MinerInfo,
    ValidatorInfo,
    TaskAssignment,
    MinerResult,
    ValidatorScore,
)

# --- Import các hàm logic đã tách ra ---
from .selection import select_miners_logic
from .scoring import score_results_logic, broadcast_scores_logic
from .state import (
    run_consensus_logic,
    verify_and_penalize_logic,
    prepare_miner_updates_logic,
    prepare_validator_updates_logic,
    commit_updates_logic,
)

# --- Logging ---
logger = logging.getLogger(__name__)

# --- Import metrics ---
from ..monitoring.metrics import metrics


class ValidatorNode:
    """
    Main coordinating class for a Validator Node.
    Manages the node's state, orchestrates the consensus cycle phases,
    and interacts with other SDK modules (metagraph, formulas, P2P, state logic).

    Attributes:
        info (ValidatorInfo): Information about this validator (UID, Address, API Endpoint).
        client (RestClient): Aptos REST client for blockchain interactions.
        account (Account): Aptos account for signing transactions.
        contract_address (str): ModernTensor contract address on Aptos.
        signing_key (Optional): The signing key for this validator (for Aptos transactions).
        settings (Settings): Centralized application settings.
        state_file (str): Path to the file storing the last completed cycle number.
        current_cycle (int): The current consensus cycle number the node is processing.
        miners_info (Dict[str, MinerInfo]): Information about known miners, loaded from the metagraph.
        validators_info (Dict[str, ValidatorInfo]): Information about known validators, loaded from the metagraph.
        tasks_sent (Dict[str, TaskAssignment]): Tracks tasks sent to miners in the current cycle.
        cycle_scores (Dict[str, List[ValidatorScore]]): Accumulates local scores assigned in the current cycle.
        miner_is_busy (Set[str]): UIDs (hex) of miners currently processing a task.
        results_buffer (Dict[str, MinerResult]): Buffer for results received from miners via API.
        results_buffer_lock (asyncio.Lock): Lock for accessing results_buffer.
        validator_scores (Dict[str, List[ValidatorScore]]): Local scores generated by this validator.
        consensus_results_cache (OrderedDict[int, CycleConsensusResults]): Cache for recent cycle consensus results.
        consensus_results_cache_lock (asyncio.Lock): Lock for accessing consensus_results_cache.
        received_validator_scores (Dict[int, Dict[str, Dict[str, ValidatorScore]]]): Stores scores received from peers.
        received_scores_lock (asyncio.Lock): Lock for accessing received_validator_scores.
        previous_cycle_results (Dict[str, Any]): Stores calculated states from the previous cycle for verification.
        http_client (httpx.AsyncClient): Async HTTP client for P2P communication.
        contract_client (AptosContractClient): Client for interacting with the ModernTensor contract on Aptos.
        metrics (Metrics): Metrics object for monitoring node performance.
        health_server (uvicorn.Server): Health check server instance.
    """

    def __init__(
        self,
        validator_info: ValidatorInfo,
        aptos_client: RestClient,
        account: Account,
        contract_address: str,
        state_file="validator_state.json",
    ):
        """
        Initializes the Validator Node.

        Args:
            validator_info (ValidatorInfo): Information for this validator (UID, Address, API Endpoint).
            aptos_client (RestClient): Aptos REST client for blockchain interactions.
            account (Account): Aptos account for signing transactions.
            contract_address (str): ModernTensor contract address on Aptos.
            state_file (str): Path to the file for saving/loading the last completed cycle.

        Raises:
            ValueError: If essential arguments are missing or invalid.
        """
        if not validator_info or not validator_info.uid:
            raise ValueError("Valid ValidatorInfo with a UID must be provided.")
        if not aptos_client:
            raise ValueError("Aptos client must be provided.")
        if not account:
            raise ValueError("Aptos account must be provided.")
        if not contract_address:
            raise ValueError("ModernTensor contract address must be provided.")

        self.info = validator_info
        # Define prefix early for use in initial logs
        self.uid_prefix = f"[{self.info.uid}]"  # Base prefix with UID
        init_prefix = f"[Init:{self.uid_prefix}]"  # Specific prefix for init

        self.client = aptos_client
        self.account = account
        self.contract_address = contract_address
        self.settings = settings
        self.state_file = state_file
        self.metrics = metrics
        self.metrics.update_memory_usage(psutil.Process().memory_info().rss)
        # Use DEBUG for file path setting
        logger.debug(f"{init_prefix} State file set to: {self.state_file}")
        self.miners_selected_for_cycle = set()

        # Load initial cycle
        self.current_cycle = self._load_last_cycle()  # This method now uses its own prefix
        self.slot_length = self.settings.CONSENSUS_CYCLE_SLOT_LENGTH

        # State variables initialization
        self.miners_info = {}
        self.validators_info = {}
        self.tasks_sent = {}
        self.cycle_scores = defaultdict(list)
        self.miner_is_busy = set()
        self.results_buffer = {}
        self.results_buffer_lock = asyncio.Lock()
        self.validator_scores = defaultdict(list)
        self.consensus_results_cache = OrderedDict()
        self.consensus_results_cache_lock = asyncio.Lock()
        self.received_validator_scores = defaultdict(lambda: defaultdict(dict))
        self.received_scores_lock = asyncio.Lock()
        self.previous_cycle_results = {}
        
        # HTTP client for network communications
        self.http_client = httpx.AsyncClient(
            timeout=self.settings.HTTP_CLIENT_TIMEOUT,
            limits=httpx.Limits(max_connections=self.settings.HTTP_CLIENT_MAX_CONNECTIONS),
        )
        
        # Import aptos client module
        from mt_aptos.aptos_core.contract_client import AptosContractClient
        
        # Create contract client for high-level Aptos interactions
        self.contract_client = AptosContractClient(
            client=self.client,
            account=self.account,
            contract_address=self.contract_address,
        )
        
        logger.info(f"{init_prefix} Validator node initialized with UID: {self.info.uid}")
        logger.info(f"{init_prefix} Connected to Aptos network with contract: {self.contract_address}")
        logger.info(f"{init_prefix} Starting at cycle: {self.current_cycle}")

        self.health_server = None

        # Initialize circuit breaker and rate limiter
        self.circuit_breaker = CircuitBreaker(
            failure_threshold=self.settings.CIRCUIT_BREAKER_FAILURE_THRESHOLD or 5,
            reset_timeout=self.settings.CIRCUIT_BREAKER_RESET_TIMEOUT or 60
        )
        self.rate_limiter = RateLimiter(
            max_requests=self.settings.RATE_LIMITER_MAX_REQUESTS or 100,
            time_window=self.settings.RATE_LIMITER_TIME_WINDOW or 60
        )

    def _load_last_cycle(self) -> int:
        """Loads the last completed cycle number from the state file."""
        # Use self.uid_prefix if self.info is already set, otherwise default
        # Ensure self.info exists before accessing uid for the prefix
        uid_val = "UnknownUID"
        if hasattr(self, "info") and self.info and hasattr(self.info, "uid"):
            uid_val = self.info.uid
        prefix = f"[bold blue][LoadState:{uid_val}][/bold blue]"
        try:
            if os.path.exists(self.state_file):
                with open(self.state_file, "r") as f:
                    state_data = json.load(f)
                    last_completed_cycle = state_data.get("last_completed_cycle", -1)
                    next_cycle = last_completed_cycle + 1
                    # <<< LOGGING ADDED >>>
                    logger.info(
                        f"{prefix} State file read: last_completed_cycle={last_completed_cycle}, calculated next_cycle={next_cycle}"
                    )
                    logger.info(
                        f"{prefix} :inbox_tray: Loaded state from [blue]{self.state_file}[/blue]. Last completed cycle: [yellow]{last_completed_cycle}[/yellow]. Starting next: [yellow]{next_cycle}[/yellow]"
                    )
                    return next_cycle
            else:
                # <<< LOGGING ADDED >>>
                logger.warning(
                    f"{prefix} State file not found at {self.state_file}. Returning 0."
                )
                logger.warning(
                    f"{prefix} :warning: State file [blue]{self.state_file}[/blue] not found. Starting from cycle 0."
                )
                return 0
        except Exception as e:
            # <<< LOGGING ADDED >>>
            logger.error(
                f"{prefix} Error reading state file {self.state_file}: {e}. Returning 0."
            )
            logger.error(
                f"{prefix} :x: Error loading state file [blue]{self.state_file}[/blue]: {e}. Starting from cycle 0."
            )
            # Optionally log traceback for debugging
            # logger.exception(f"{prefix} Traceback for state file loading error:")
            return 0

    def _save_current_cycle(self, completed_cycle: int):
        """Saves the *just completed* cycle number to the state file."""
        # Ensure self.info exists before accessing uid for the prefix
        uid_val = "UnknownUID"
        if hasattr(self, "info") and self.info and hasattr(self.info, "uid"):
            uid_val = self.info.uid
        prefix = f"[bold magenta][SaveState:{uid_val}][/bold magenta]"
        if completed_cycle < 0:
            logger.debug(
                f"{prefix} No cycle completed yet ({completed_cycle}), skipping state save."
            )
            return

        state_data = {"last_completed_cycle": completed_cycle}
        try:
            # Ensure directory exists before writing
            os.makedirs(os.path.dirname(self.state_file) or ".", exist_ok=True)
            with open(self.state_file, "w") as f:
                json.dump(state_data, f, indent=2)  # Add indent for readability
            # Use success icon and green color
            logger.info(
                f"{prefix} :outbox_tray: Saved last completed cycle [yellow]{completed_cycle}[/yellow] to [blue]{self.state_file}[/blue]"
            )
        except Exception as e:
            # Use error icon and red color
            logger.error(
                f"{prefix} :x: Error saving state to [blue]{self.state_file}[/blue]: {e}"
            )
            # Optionally log traceback for debugging
            # logger.exception(f"{prefix} Traceback for state file saving error:")

    # --- Thêm phương thức mới để lấy kết quả từ cache ---
    async def get_consensus_results_for_cycle(
        self, cycle_num: int
    ) -> Optional[CycleConsensusResults]:
        """Retrieves cached consensus results for a specific cycle."""
        async with self.consensus_results_cache_lock:
            return self.consensus_results_cache.get(cycle_num)

    # --- Thêm phương thức mới để công bố/lưu kết quả ---
    async def _publish_consensus_results(
        self,
        cycle: int,
        final_miner_scores: Dict[str, float],
        calculated_rewards: Dict[str, float],
    ):
        """
        Caches the consensus results for API access.
        (Placeholder: Future extensions could include signing, IPFS upload, or on-chain hash commit).

        Args:
            cycle (int): The cycle number these results belong to.
            final_miner_scores (Dict[str, float]): Final adjusted performance scores for miners.
            calculated_rewards (Dict[str, float]): Calculated incentive rewards for miners.
        """
        logger.info(
            f"[V:{self.info.uid}] Caching consensus results for cycle {cycle}..."
        )
        results_for_miners: Dict[str, MinerConsensusResult] = {}

        # Lấy tất cả miner IDs từ final_scores hoặc calculated_rewards
        all_miner_ids = set(final_miner_scores.keys()) | set(calculated_rewards.keys())

        for miner_uid_hex in all_miner_ids:
            p_adj = final_miner_scores.get(
                miner_uid_hex, 0.0
            )  # Mặc định 0 nếu không có điểm
            incentive = calculated_rewards.get(
                miner_uid_hex, 0.0
            )  # Mặc định 0 nếu không có thưởng

            # Tạo đối tượng kết quả cho miner này
            # Note: hiện tại chưa tính sẵn trust_score mới cho miner ở đây
            # vì việc đó miner sẽ tự làm khi cập nhật.
            miner_result = MinerConsensusResult(
                miner_uid=miner_uid_hex, p_adj=p_adj, calculated_incentive=incentive
            )
            results_for_miners[miner_uid_hex] = miner_result

        # Tạo đối tượng kết quả cho cả chu kỳ
        cycle_results = CycleConsensusResults(
            cycle=cycle,
            results=results_for_miners,
            publisher_uid=(
                self.info.uid.hex()
                if isinstance(self.info.uid, bytes)
                else self.info.uid
            ),  # Đã bỏ comment và thêm uid
            # signature=... # Thêm chữ ký sau
        )

        # Lưu vào cache (dùng OrderedDict để giới hạn kích thước)
        async with self.consensus_results_cache_lock:
            self.consensus_results_cache[cycle] = cycle_results
            # Giữ cache trong giới hạn kích thước
            while len(self.consensus_results_cache) > self.max_cache_cycles:
                self.consensus_results_cache.popitem(last=False)  # Xóa item cũ nhất

        logger.info(
            f"Consensus results for cycle {cycle} cached ({len(results_for_miners)} miners). Ready for API access."
        )
        # TODO (Future): Implement actual publication (IPFS, On-chain hash, Signed API)

    # --- Tương tác Metagraph ---
    async def load_metagraph_data(self):
        """
        Tải dữ liệu miner và validator từ blockchain Aptos.

        Lấy tất cả thông tin miner và validator từ smart contract ModernTensor,
        phân tích dữ liệu, xác minh lịch sử hiệu suất so với trạng thái cục bộ (nếu có),
        và cập nhật trạng thái nội bộ của node (self.miners_info, self.validators_info).

        Raises:
            RuntimeError: Nếu xảy ra lỗi nghiêm trọng khi lấy hoặc xử lý dữ liệu,
                          khiến node không thể tiếp tục chu kỳ.
        """
        logger.info(
            f"[V:{self.info.uid}] Đang tải dữ liệu từ Aptos blockchain cho chu kỳ {self.current_cycle}..."
        )
        start_time = time.time()

        # Lưu trữ trạng thái miners và validators trước đó để so sánh
        previous_miners_info = self.miners_info.copy()
        previous_validators_info = self.validators_info.copy()

        temp_miners_info = {}
        temp_validators_info = {}
        try:
            # Gọi song song để lấy dữ liệu miners và validators từ Aptos
            from mt_aptos.aptos_core.validator_helper import get_all_validators, get_all_miners
            
            miner_data_task = get_all_miners(
                client=self.client,
                contract_address=self.contract_address
            )
            validator_data_task = get_all_validators(
                client=self.client,
                contract_address=self.contract_address
            )
                
            # Đợi các task hoàn thành
            miners_data, validators_data = await asyncio.gather(
                miner_data_task, validator_data_task, return_exceptions=True
            )

            # Xử lý lỗi fetch
            if isinstance(miners_data, Exception):
                logger.error(f"Lỗi khi lấy dữ liệu miners: {miners_data}")
                miners_data = []
            if isinstance(validators_data, Exception):
                logger.error(f"Lỗi khi lấy dữ liệu validators: {validators_data}")
                validators_data = []

            logger.info(f"Đã lấy {len(miners_data)} miner và {len(validators_data)} validator.")

            # --- Chuyển đổi dữ liệu miners thành MinerInfo ---
            for miner_data in miners_data:
                try:
                    uid_hex = miner_data.get("uid")
                    if not uid_hex:
                        continue

                    # Lấy history hash từ dữ liệu (nếu có)
                    on_chain_history_hash_hex = miner_data.get("performance_history_hash")
                    on_chain_history_hash_bytes = (
                        bytes.fromhex(on_chain_history_hash_hex)
                        if on_chain_history_hash_hex
                        else None
                    )

                    # Lấy lịch sử hiệu suất từ thông tin cũ (nếu có)
                    current_local_history = []  # Mặc định là rỗng
                    previous_info = previous_miners_info.get(uid_hex)
                    if previous_info:
                        current_local_history = previous_info.performance_history  # Lấy lịch sử cũ từ bộ nhớ

                    # Xác minh lịch sử hiệu suất
                    verified_history = []  # Lịch sử sẽ được lưu vào MinerInfo mới
                    if on_chain_history_hash_bytes:
                        # Nếu có hash on-chain, thử xác minh lịch sử cục bộ
                        if current_local_history:
                            try:
                                local_history_hash = hash_data(current_local_history)
                                if local_history_hash == on_chain_history_hash_bytes:
                                    verified_history = current_local_history  # Hash khớp, giữ lại lịch sử cục bộ
                                    logger.debug(
                                        f"Miner {uid_hex}: Local history verified against on-chain hash."
                                    )
                                else:
                                    logger.warning(
                                        f"Miner {uid_hex}: Local history hash mismatch! Resetting history."
                                    )
                                    verified_history = []  # Hash không khớp, reset
                            except Exception as hash_err:
                                logger.error(
                                    f"Miner {uid_hex}: Error hashing local history: {hash_err}. Resetting history."
                                )
                                verified_history = []
                        else:
                            logger.warning(
                                f"Miner {uid_hex}: On-chain history hash found, but no local history available. Resetting history."
                            )
                            verified_history = []
                    else:
                        logger.debug(
                            f"Miner {uid_hex}: No on-chain history hash found. Using current local history."
                        )
                        verified_history = current_local_history  # Giữ lại lịch sử cục bộ (thường là rỗng)

                    # Đảm bảo giới hạn độ dài lịch sử
                    max_history_len = settings.CONSENSUS_MAX_PERFORMANCE_HISTORY_LEN
                    verified_history = verified_history[-max_history_len:]

                    # Lấy wallet_addr_hash nếu có
                    wallet_addr_hash_hex = miner_data.get("wallet_addr_hash")
                    wallet_addr_hash_bytes = (
                        bytes.fromhex(wallet_addr_hash_hex)
                        if wallet_addr_hash_hex
                        else None
                    )

                    # Tạo đối tượng MinerInfo
                    temp_miners_info[uid_hex] = MinerInfo(
                        uid=uid_hex,
                        address=miner_data.get("address", f"0x{uid_hex[:8]}..."),
                        api_endpoint=miner_data.get("api_endpoint"),
                        trust_score=float(miner_data.get("trust_score", 0.0)),
                        weight=float(miner_data.get("weight", 0.0)),
                        stake=float(miner_data.get("stake", 0)),
                        last_selected_time=int(miner_data.get("last_selected_time", -1)),
                        performance_history=verified_history,
                        subnet_uid=int(miner_data.get("subnet_uid", -1)),
                        status=int(miner_data.get("status", STATUS_INACTIVE)),
                        registration_slot=int(miner_data.get("registration_slot", 0)),
                        wallet_addr_hash=wallet_addr_hash_bytes,
                        performance_history_hash=on_chain_history_hash_bytes,
                    )

                except Exception as e:
                    logger.warning(
                        f"Lỗi khi phân tích dữ liệu Miner cho UID {miner_data.get('uid', 'N/A')}: {e}",
                        exc_info=False,
                    )
                    logger.debug(f"Dữ liệu miner gặp vấn đề: {miner_data}")

            # --- Tương tự, chuyển đổi dữ liệu validator thành ValidatorInfo ---
            for validator_data in validators_data:
                try:
                    uid_hex = validator_data.get("uid")
                    if not uid_hex:
                        continue

                    # Xử lý tương tự như với miners để lấy và xác minh history
                    on_chain_history_hash_hex = validator_data.get("performance_history_hash")
                    on_chain_history_hash_bytes = (
                        bytes.fromhex(on_chain_history_hash_hex)
                        if on_chain_history_hash_hex
                        else None
                    )

                    current_local_history = []
                    previous_info = previous_validators_info.get(uid_hex)
                    if previous_info and hasattr(previous_info, "performance_history"):
                        current_local_history = previous_info.performance_history

                    verified_history = []
                    # Xác minh history tương tự như với miners
                    if on_chain_history_hash_bytes:
                        if current_local_history:
                            try:
                                local_history_hash = hash_data(current_local_history)
                                if local_history_hash == on_chain_history_hash_bytes:
                                    verified_history = current_local_history
                                    logger.debug(
                                        f"Validator {uid_hex}: Local history verified."
                                    )
                                else:
                                    logger.warning(
                                        f"Validator {uid_hex}: History hash mismatch! Resetting."
                                    )
                                    verified_history = []
                            except Exception as hash_err:
                                logger.error(
                                    f"Validator {uid_hex}: Error hashing local history: {hash_err}. Resetting."
                                )
                                verified_history = []
                        else:
                            logger.warning(
                                f"Validator {uid_hex}: On-chain hash found, no local history. Resetting."
                            )
                            verified_history = []
                    else:
                        logger.debug(
                            f"Validator {uid_hex}: No on-chain history hash. Using current local."
                        )
                        verified_history = current_local_history

                    verified_history = verified_history[-max_history_len:]

                    # --- Lấy address bytes từ datum và decode ---
                    # Lấy các hash khác
                    wallet_addr_hash_hex = validator_data.get("wallet_addr_hash")
                    wallet_addr_hash_bytes = (
                        bytes.fromhex(wallet_addr_hash_hex)
                        if wallet_addr_hash_hex
                        else None
                    )
                    # ------------------------------------------

                    # <<< ADDED: Sanitize api_endpoint >>>
                    raw_endpoint = validator_data.get("api_endpoint")
                    clean_endpoint: Optional[str] = None
                    if isinstance(raw_endpoint, str):
                        # Basic check for common protocols and printable chars
                        if raw_endpoint.startswith(("http://", "https://")) and all(
                            c in string.printable for c in raw_endpoint
                        ):
                            clean_endpoint = raw_endpoint
                        else:
                            logger.warning(
                                f"Validator {uid_hex}: Invalid format or characters in api_endpoint: '{raw_endpoint}'. Setting to None."
                            )
                    elif raw_endpoint is not None:
                        logger.warning(
                            f"Validator {uid_hex}: api_endpoint is not a string (type: {type(raw_endpoint)}). Setting to None."
                        )
                    # <<< END ADDED >>>

                    temp_validators_info[uid_hex] = ValidatorInfo(
                        uid=uid_hex,
                        address=validator_data.get(
                            "address", f"addr_validator_{uid_hex[:8]}..."
                        ),
                        api_endpoint=clean_endpoint,  # <<< Use sanitized endpoint
                        trust_score=float(validator_data.get("trust_score", 0.0)),
                        weight=float(validator_data.get("weight", 0.0)),
                        stake=float(validator_data.get("stake", 0)),
                        last_performance=float(validator_data.get("last_performance", 0.0)),
                        performance_history=verified_history,
                        subnet_uid=int(validator_data.get("subnet_uid", -1)),
                        status=int(validator_data.get("status", STATUS_INACTIVE)),
                        registration_slot=int(validator_data.get("registration_slot", 0)),
                        wallet_addr_hash=wallet_addr_hash_bytes,
                        performance_history_hash=on_chain_history_hash_bytes,  # Lưu lại hash on-chain
                    )
                    logger.debug(
                        f"  Loaded Validator Peer: UID={uid_hex}, Status={validator_data.get('status', 'N/A')}, Endpoint='{validator_data.get('api_endpoint', 'N/A')}'"
                    )
                except Exception as e:
                    logger.warning(
                        f"Failed to parse Validator data dict for UID {validator_data.get('uid', 'N/A')}: {e}",
                        exc_info=False,
                    )
                    logger.debug(f"Problematic validator data dict: {validator_data}")

            # --- Cập nhật trạng thái node ---
            self.miners_info = temp_miners_info
            self.validators_info = temp_validators_info

            # Cập nhật thông tin của chính mình
            self_uid_hex = (
                self.info.uid.hex()
                if isinstance(self.info.uid, bytes)
                else self.info.uid
            )
            if self_uid_hex in self.validators_info:
                loaded_info = self.validators_info[self_uid_hex]
                #  self.info.address = loaded_info.address
                self.info.api_endpoint = loaded_info.api_endpoint
                self.info.trust_score = loaded_info.trust_score
                self.info.weight = loaded_info.weight
                self.info.stake = loaded_info.stake
                # Cập nhật thêm các trường khác nếu cần
                logger.info(
                    f"Self validator info ({self_uid_hex}) updated from metagraph."
                )
            elif self.info.uid:
                self.validators_info[self_uid_hex] = self.info
                logger.warning(
                    f"Self validator ({self_uid_hex}) not found in metagraph, added locally. Ensure initial state is correct."
                )
            else:
                logger.error(
                    "Current validator info UID is invalid after loading metagraph."
                )

            # TODO: Load và xử lý dữ liệu Subnet/Foundation nếu cần

            load_duration = time.time() - start_time
            logger.info(
                f"Processed info for {len(self.miners_info)} miners and {len(self.validators_info)} validators in {load_duration:.2f}s."
            )

        except Exception as e:
            logger.exception(
                f"Critical error during metagraph data loading/processing: {e}. Cannot proceed this cycle."
            )
            # Initialize empty objects on error
            self.miners_info = {}
            self.validators_info = {}
            raise RuntimeError(f"Failed to load and process metagraph data: {e}") from e

    # --- Lựa chọn Miner ---
    def select_miners(self) -> List[MinerInfo]:
        """Selects miners for task assignment based on configured logic."""
        logger.info(
            f"[V:{self.info.uid}] Selecting miners for cycle {self.current_cycle}..."
        )
        num_to_select = self.settings.CONSENSUS_NUM_MINERS_TO_SELECT
        beta = self.settings.CONSENSUS_PARAM_BETA
        max_time_bonus = self.settings.CONSENSUS_PARAM_MAX_TIME_BONUS
        # Gọi hàm logic từ selection.py
        return select_miners_logic(
            miners_info=self.miners_info,
            current_cycle=self.current_cycle,
            num_to_select=num_to_select,  # Truyền số lượng cần chọn
            beta=beta,  # Truyền hệ số beta
            max_time_bonus=max_time_bonus,  # Truyền giới hạn bonus thời gian
        )

    def _select_available_miners_for_batch(self, num_to_select: int) -> List[MinerInfo]:
        """
        Selects a batch of available (not busy) and active miners using the main selection logic.

        Args:
            num_to_select (int): Desired number of miners for the batch (N).

        Returns:
            List[MinerInfo]: A list of selected available miners, up to num_to_select.
                             Returns an empty list if no suitable miners are found.
        """
        # 1. Lọc các miner đang hoạt động (active)
        active_miners_all = [
            m
            for m in self.miners_info.values()
            if getattr(m, "status", STATUS_ACTIVE) == STATUS_ACTIVE
        ]
        if not active_miners_all:
            logger.debug("Mini-batch selection: No active miners found.")
            return []

        # 2. Lọc tiếp các miner không bận (uid không có trong self.miner_is_busy)
        available_miners = [
            m for m in active_miners_all if m.uid not in self.miner_is_busy
        ]

        if not available_miners:
            # Không log warning vì đây là trường hợp bình thường khi chờ miner xử lý xong
            logger.debug(
                "Mini-batch selection: No available (not busy) active miners found at the moment."
            )
            return []

        logger.debug(
            f"Mini-batch selection: {len(available_miners)} available miners to choose from."
        )

        # 3. Chuẩn bị đầu vào cho logic lựa chọn chính (cần một dict)
        available_miners_dict = {m.uid: m for m in available_miners}

        # 4. Lấy các tham số lựa chọn từ settings
        beta = self.settings.CONSENSUS_PARAM_BETA
        max_time_bonus = self.settings.CONSENSUS_PARAM_MAX_TIME_BONUS

        # 5. Gọi logic lựa chọn hiện có (select_miners_logic) trên nhóm miner khả dụng
        #    Giới hạn số lượng chọn bằng số lượng thực tế khả dụng
        actual_num_to_select = min(num_to_select, len(available_miners))
        if actual_num_to_select <= 0:
            return []  # Không có ai để chọn

        # Gọi hàm logic từ selection.py
        selected_miners_for_batch = select_miners_logic(
            miners_info=available_miners_dict,
            current_cycle=self.current_cycle,  # Sử dụng cycle hiện tại để tính bonus time
            num_to_select=actual_num_to_select,  # Chọn tối đa N
            beta=beta,
            max_time_bonus=max_time_bonus,
        )

        logger.debug(
            f"Mini-batch selection: Selected {len(selected_miners_for_batch)} miners for this batch: {[m.uid for m in selected_miners_for_batch]}"
        )
        return selected_miners_for_batch

    # --- Giao Task ---
    # --- 1. Đánh dấu _create_task_data ---
    def _create_task_data(self, miner_uid: str) -> Any:
        """
        (Abstract/Needs Override) Creates specific task data for a miner.

        Inheriting Validator classes for each Subnet MUST override this method
        to define the task content and format appropriate for the AI problem.

        Args:
            miner_uid (str): The UID of the miner receiving the task.

        Returns:
            Any: The task data (should be JSON-serializable).

        Raises:
            NotImplementedError: If not overridden by the subclass.
        """
        logger.error(
            f"'_create_task_data' must be implemented by the inheriting Validator class for miner {miner_uid}."
        )
        raise NotImplementedError(
            "Subnet Validator must implement task creation logic."
        )

    # --- Phương thức helper mới để gửi task lô ---
    # async def _send_task_batch(
    #     self, miners_to_send: List[MinerInfo], batch_num: int
    # ) -> Dict[str, TaskAssignment]:
    #     """Gửi một lô task đến các miner được chỉ định và trả về dict các task đã gửi thành công."""
    #     tasks_to_send_coroutines = []
    #     tasks_sent_successfully: Dict[str, TaskAssignment] = {}
    #     miners_sent_to = []  # Giữ thứ tự để khớp với results

    #     if not miners_to_send:
    #         return tasks_sent_successfully

    #     logger.info(f"Preparing to send task batch to {len(miners_to_send)} miners...")

    #     for miner_info in miners_to_send:
    #         miner_uid = miner_info.uid
    #         # Đánh dấu bận ngay
    #         self.miner_is_busy.add(miner_uid)

    #         # Tạo task ID duy nhất (thêm timestamp nhỏ hoặc số thứ tự batch nếu cần)
    #         task_id = f"task_{self.current_cycle}_{self.info.uid}_{miner_uid}_b{batch_num}_{random.randint(1000,9999)}"
    #         try:
    #             task_data = self._create_task_data(miner_uid)
    #             if task_data is None:
    #                 raise ValueError("_create_task_data returned None")
    #             task = TaskModel(task_id=task_id, **task_data)
    #         except Exception as e:
    #             logger.exception(f"Failed to create task for miner {miner_uid}: {e}")
    #             self.miner_is_busy.discard(miner_uid)  # Hủy đánh dấu bận
    #             continue  # Bỏ qua miner này

    #         assignment = TaskAssignment(
    #             task_id=task_id,
    #             task_data=task_data,
    #             miner_uid=miner_uid,
    #             validator_uid=self.info.uid,
    #             timestamp_sent=time.time(),
    #             expected_result_format={},  # Cập nhật nếu cần
    #         )
    #         self.tasks_sent[task_id] = assignment  # Thêm vào danh sách chờ tổng
    #         tasks_sent_successfully[task_id] = assignment  # Thêm vào danh sách lô này
    #         miners_sent_to.append(miner_info)  # Lưu lại miner đã gửi

    #         tasks_to_send_coroutines.append(
    #             self._send_task_via_network_async(miner_info.api_endpoint, task)  # type: ignore
    #         )

    #     # Gửi task đi
    #     if tasks_to_send_coroutines:
    #         logger.info(
    #             f"Sending batch of {len(tasks_to_send_coroutines)} tasks concurrently..."
    #         )
    #         results = await asyncio.gather(
    #             *tasks_to_send_coroutines, return_exceptions=True
    #         )

    #         # Xử lý lỗi gửi
    #         success_count = 0
    #         for i, result in enumerate(results):
    #             if i < len(miners_sent_to):
    #                 miner_info_sent = miners_sent_to[i] # Lấy miner theo đúng thứ tự gửi
    #                 # Tìm assignment tương ứng trong lô đã gửi thành công
    #                 assignment_key_to_find = None
    #                 for task_id_key, assign_val in tasks_sent_successfully.items():
    #                     if assign_val.miner_uid == miner_info_sent.uid:
    #                         # Giả định chỉ gửi 1 task/miner/batch, nếu gửi nhiều cần logic phức tạp hơn
    #                         assignment_key_to_find = task_id_key
    #                         break

    #                 if assignment_key_to_find:
    #                     assign = tasks_sent_successfully[assignment_key_to_find]
    #                     if isinstance(result, bool) and result:
    #                         success_count += 1
    #                     else:
    #                         logger.warning(f"Failed send task {assign.task_id} to {assign.miner_uid}: {result}. Marking available.")
    #                         self.miner_is_busy.discard(assign.miner_uid)
    #                         if assign.task_id in self.tasks_sent: del self.tasks_sent[assign.task_id]
    #                         del tasks_sent_successfully[assign.task_id] # Xóa khỏi dict trả về
    #                 else:
    #                     # Lỗi logic: Không tìm thấy assignment cho miner đã gửi?
    #                     logger.error(f"Could not find assignment for miner {miner_info_sent.uid} in successfully sent batch after gather.")

    #             else: logger.error("Result index mismatch during send processing.")
    #     else:
    #         logger.warning("No tasks were actually sent in this batch.")

    #     return tasks_sent_successfully

    async def _send_task_batch(
        self, miners_for_batch: List[MinerInfo], batch_num: int
    ) -> Dict[str, TaskAssignment]:
        """
        Creates and sends tasks to a specific batch of miners asynchronously.
        Marks miners as busy and tracks successfully sent tasks.

        Args:
            miners_for_batch: List of MinerInfo objects selected for this batch.
            batch_num: The sequence number of this mini-batch within the cycle.

        Returns:
            A dictionary {task_id: TaskAssignment} for tasks successfully sent
            in this batch. Returns an empty dict if no tasks could be sent.
        """
        if not miners_for_batch:
            logger.info(f"Batch {batch_num}: No miners provided for task sending.")
            return {}

        logger.info(
            f"Batch {batch_num}: Preparing to send tasks to {len(miners_for_batch)} miners..."
        )

        tasks_to_send_coroutines = []
        # Temporary dict to track assignments only for THIS batch before sending
        batch_assignments: Dict[str, TaskAssignment] = {}
        # Keep track of miner UIDs we actually attempt to send to in this batch
        miners_sent_attempted_uids = []

        for miner_info in miners_for_batch:
            miner_uid = miner_info.uid
            # Basic check (already done in selection, but good for safety)
            if not miner_info.api_endpoint or not miner_info.api_endpoint.startswith(
                ("http://", "https://")
            ):
                logger.warning(
                    f"Batch {batch_num}: Miner {miner_uid} has invalid API endpoint ('{miner_info.api_endpoint}'). Skipping."
                )
                continue

            # Create unique task ID including batch number
            # Ensure validator UID is hex string
            self_uid_hex = (
                self.info.uid.hex()
                if isinstance(self.info.uid, bytes)
                else self.info.uid
            )
            task_id = f"task_{self.current_cycle}_{self_uid_hex}_{miner_uid}_b{batch_num}_{random.randint(1000,9999)}"

            try:
                # Create task data using the overridable method
                task_data = self._create_task_data(miner_uid)
                if task_data is None:  # Ensure task data was created
                    raise ValueError("_create_task_data returned None")
                # Assume TaskModel can take task_id and unpack task_data dict
                task = TaskModel(task_id=task_id, **task_data)
            except NotImplementedError:
                logger.error(
                    f"CRITICAL: _create_task_data not implemented by subclass for miner {miner_uid}! Cannot send task."
                )
                continue  # Skip this miner
            except Exception as e:
                logger.exception(
                    f"Batch {batch_num}: Failed to create task for miner {miner_uid}: {e}"
                )
                continue  # Skip this miner

            # Create TaskAssignment
            assignment = TaskAssignment(
                task_id=task_id,
                task_data=task_data,
                miner_uid=miner_uid,
                validator_uid=self_uid_hex,  # Use hex UID
                timestamp_sent=time.time(),
                expected_result_format={},  # TODO: Define actual expected format if needed
            )

            # --- State Updates Before Sending ---
            self.miner_is_busy.add(miner_uid)  # Mark as busy immediately
            self.tasks_sent[task_id] = assignment  # Add to overall tracking
            batch_assignments[task_id] = assignment  # Add to this batch's tracking
            miners_sent_attempted_uids.append(
                miner_uid
            )  # Track UID for result processing
            # ------------------------------------

            # Prepare the coroutine for sending
            tasks_to_send_coroutines.append(
                self._send_task_via_network_async(miner_info.api_endpoint, task)
            )
            logger.debug(
                f"Batch {batch_num}: Prepared task {task_id} for miner {miner_uid}."
            )

        # --- Send Tasks Concurrently ---
        if not tasks_to_send_coroutines:
            logger.warning(
                f"Batch {batch_num}: No valid tasks could be prepared for sending."
            )
            return {}  # Return empty dict if nothing was prepared

        logger.info(
            f"Batch {batch_num}: Sending {len(tasks_to_send_coroutines)} tasks concurrently..."
        )
        # Gather results (True for success, False/Exception for failure)
        send_results = await asyncio.gather(
            *tasks_to_send_coroutines, return_exceptions=True
        )

        # --- Process Send Results ---
        successful_sends_in_batch = 0
        failed_assignment_keys_in_batch = []

        # Iterate through results, matching them back to miners sent in this batch
        for i, send_result in enumerate(send_results):
            # Ensure index is valid
            if i >= len(miners_sent_attempted_uids):
                logger.error(
                    f"Batch {batch_num}: Result index {i} mismatch with attempted miners."
                )
                continue

            miner_uid_sent = miners_sent_attempted_uids[i]
            # Find the corresponding task_id in this batch
            task_id_for_miner = None
            for tid, assign in batch_assignments.items():
                if assign.miner_uid == miner_uid_sent:
                    task_id_for_miner = tid
                    break

            if not task_id_for_miner:
                logger.error(
                    f"Batch {batch_num}: Could not find assignment for miner {miner_uid_sent} in this batch's tracking."
                )
                continue

            # Check if sending was successful
            if isinstance(send_result, bool) and send_result:
                successful_sends_in_batch += 1
                logger.debug(
                    f"Batch {batch_num}: Successfully sent task {task_id_for_miner} to miner {miner_uid_sent}."
                )
            else:
                # Sending failed
                logger.warning(
                    f"Batch {batch_num}: Failed sending task {task_id_for_miner} to miner {miner_uid_sent}. Error: {send_result}"
                )
                # --- Revert State Updates for Failed Send ---
                self.miner_is_busy.discard(miner_uid_sent)  # Mark as not busy anymore
                if task_id_for_miner in self.tasks_sent:
                    del self.tasks_sent[
                        task_id_for_miner
                    ]  # Remove from overall tracking
                # Mark for removal from the batch's successful assignments
                failed_assignment_keys_in_batch.append(task_id_for_miner)
                # -------------------------------------------

        # Remove failed assignments from the dictionary to be returned
        for task_id_to_remove in failed_assignment_keys_in_batch:
            if task_id_to_remove in batch_assignments:
                del batch_assignments[task_id_to_remove]

        logger.info(
            f"Batch {batch_num}: Sending attempt finished. Successful sends in this batch: {successful_sends_in_batch}/{len(tasks_to_send_coroutines)}."
        )

        # Return only the assignments for tasks successfully sent in this batch
        return batch_assignments

    # --- Phương thức chấm điểm lô mới ---
    # def _score_batch_results(self, tasks_in_batch: Dict[str, TaskAssignment]): # Commented out older version
    #     """
    #     Chấm điểm các kết quả trong buffer tương ứng với các task trong lô này.
    #     Gán điểm 0 cho các task không có kết quả trong buffer (timeout).
    #     Cập nhật self.cycle_scores và giải phóng miner khỏi self.miner_is_busy.
    #     Xóa task đã xử lý khỏi self.tasks_sent.
    #     """
    #     ...

    #     # --- Thêm phương thức này vào class ValidatorNode ---

    # def _score_individual_result(self, task_data: Any, result_data: Any) -> float: # Commented out older version
    #     """
    #     Placeholder cho logic chấm điểm của Subnet cụ thể.
    #     Phương thức này BẮT BUỘC phải được override bởi lớp Validator kế thừa.
    #     """
    #     ...

    #     # --- Add this scoring method specific to mini-batches ---

    async def _score_current_batch(self, batch_assignments: Dict[str, TaskAssignment]):
        """
        Scores results for a completed mini-batch, handling timeouts.

        Retrieves results from the buffer, calls the subnet-specific
        `_score_individual_result` logic, appends scores (including 0.0 for timeouts)
        to `self.cycle_scores`, releases miners from the busy set, and removes
        processed tasks from `self.tasks_sent`.

        Args:
            batch_assignments: Dict {task_id: TaskAssignment} for the batch just finished.

        Raises:
            NotImplementedError: If `_score_individual_result` is not implemented by the subclass.
            Exception: If errors occur during the scoring logic itself.
        """
        batch_num_str = (
            ""  # Try to extract batch number for logging if available in task_id
        )
        if batch_assignments:
            first_task_id = next(iter(batch_assignments.keys()))
            parts = first_task_id.split("_b")
            if len(parts) > 1:
                num_part = parts[-1].split("_")[0]
                if num_part.isdigit():
                    batch_num_str = f" (Batch ~{num_part})"

        logger.info(
            f"Scoring results for {len(batch_assignments)} tasks{batch_num_str}..."
        )
        scores_added_count = 0
        timeouts_count = 0

        # Atomically get and clear the current results buffer
        # NOTE: This assumes add_miner_result uses the same lock. If not, acquire lock here.
        # If add_miner_result is sync and called from API thread, this might be okay without async lock.
        # Let's add the lock for safety assuming potential concurrency.
        # This method itself runs synchronously within the main async run_cycle loop.
        # The lock is needed because add_miner_result (called by API) might run concurrently.
        async with self.results_buffer_lock:  # Use the sync version of the lock
            buffered_results_copy = self.results_buffer.copy()
            self.results_buffer.clear()
            logger.debug(
                f"Copied and cleared results buffer. Size was: {len(buffered_results_copy)}"
            )

        # Get validator UID (should be hex string already)
        self_uid_hex = (
            self.info.uid.hex() if isinstance(self.info.uid, bytes) else self.info.uid
        )

        # Iterate through the tasks EXPECTED in this batch
        for task_id, assignment in batch_assignments.items():
            miner_uid = assignment.miner_uid
            score = 0.0
            result_found = False

            # Check if a result for this task_id arrived in the buffer
            if task_id in buffered_results_copy:
                result = buffered_results_copy[task_id]
                result_found = True

                # Double-check miner UID (should be correct if add_miner_result checked)
                if result.miner_uid == miner_uid:
                    try:
                        # ===>>> CALL THE SUBNET-SPECIFIC SCORING LOGIC HERE <<<===
                        # This method MUST be overridden by the specific Validator subclass
                        # (e.g., Subnet1Validator needs to implement this or ensure its
                        # scoring logic is accessible via this call).
                        score = self._score_individual_result(
                            assignment.task_data, result.result_data
                        )
                        logger.info(
                            f"  Task {task_id}: Raw score calculated = {score:.4f} for Miner {miner_uid}"
                        )

                        # Clamp score
                        score = max(0.0, min(1.0, score))
                        scores_added_count += 1
                        logger.debug(
                            f"  Task {task_id}: Scored result from miner {miner_uid}. Score: {score:.4f}"
                        )

                    except NotImplementedError:
                        logger.error(
                            f"CRITICAL: Scoring logic '_score_individual_result' not implemented in {self.__class__.__name__} for task {task_id}! Assigning score 0."
                        )
                        score = 0.0  # Assign 0 if scoring is not implemented
                    except Exception as e:
                        logger.exception(
                            f"  Task {task_id}: Error scoring result from miner {miner_uid}: {e}. Assigning score 0."
                        )
                        score = 0.0  # Assign 0 on scoring error
                else:
                    # This case indicates a potential logic error in how results are buffered/matched
                    logger.error(
                        f"  Task {task_id}: Result in buffer from unexpected miner {result.miner_uid} (expected {miner_uid}). Assigning score 0."
                    )
                    score = 0.0
            else:
                # Timeout for this batch
                timeouts_count += 1
                score = 0.0  # Assign score 0 for timeout
                logger.warning(
                    f"  Task {task_id}: Timeout - No result received from miner {miner_uid} within batch wait time. Assigning score 0."
                )

            # Create ValidatorScore object
            val_score = ValidatorScore(
                task_id=task_id,
                miner_uid=miner_uid,
                validator_uid=self_uid_hex,
                score=score,
                timestamp=time.time(),  # Timestamp of scoring
            )

            # Append score to the main cycle accumulator
            # Use defaultdict initialized in __init__
            self.cycle_scores[task_id].append(val_score)

            # --- Release Miner and Task Tracking ---
            self.miner_is_busy.discard(miner_uid)  # Mark miner as available
            if task_id in self.tasks_sent:
                del self.tasks_sent[task_id]  # Remove from overall pending tasks
            # ---------------------------------------

        logger.info(
            f"Batch scoring complete{batch_num_str}. Scores added: {scores_added_count}. Timeouts (Score 0): {timeouts_count}."
        )

    # --- Add this placeholder scoring method (IMPORTANT) ---
    # This NEEDS to be overridden by Subnet1Validator
    def _score_individual_result(self, task_data: Any, result_data: Any) -> float:
        """
        (Placeholder/Needs Override) Calculates the score for a single miner result.

        *** This method MUST be implemented by the specific Validator subclass ***
        (e.g., Subnet1Validator) to contain the actual scoring logic based on the
        task and the received result (e.g., calling calculate_clip_score).

        Args:
            task_data (Any): Data originally sent in the task.
            result_data (Any): Data received from the miner.

        Returns:
            float: Score between 0.0 and 1.0.

        Raises:
            NotImplementedError: By default, if not overridden.
        """
        logger.error(
            f"CRITICAL: Validator {getattr(self, 'info', {}).get('uid', 'UNKNOWN')} is using the base "
            f"'_score_individual_result'. Subnet scoring logic is missing! Returning score 0.0."
        )
        # In a real scenario, either raise NotImplementedError or implement base logic
        raise NotImplementedError("Subclasses must implement _score_individual_result")
        # return 0.0 # Unreachable after raise

    # --- Tương tác với Aptos chain ---
    async def _get_current_slot(self) -> Optional[int]:
        """
        Lấy giá trị slot hiện tại dựa trên timestamp của block Aptos mới nhất.
        
        Returns:
            Optional[int]: Giá trị slot tương đương, None nếu có lỗi.
        """
        try:
            timestamp = await self._get_current_block_timestamp()
            if timestamp is None:
                return None
                
            # Tính slot giả lập dựa trên timestamp (1 slot = 1 giây)
            slot_length = self.settings.CONSENSUS_CYCLE_SLOT_LENGTH or 1  # seconds per slot
            estimated_slot = timestamp // slot_length
            
            return estimated_slot
        except Exception as e:
            logger.error(f"Lỗi khi tính toán giá trị slot hiện tại: {e}")
            return None

    async def wait_until_slot(self, target_slot: int):
        """
        Đợi cho đến khi slot hiện tại (tính từ timestamp) đạt đến hoặc vượt qua target_slot.
        
        Args:
            target_slot (int): Giá trị slot đích cần đợi.
        """
        if target_slot <= 0:
            logger.warning(
                f"wait_until_slot được gọi với giá trị target_slot không hợp lệ: {target_slot}"
            )
            return

        logger.debug(f"Đợi cho đến slot {target_slot}...")
        
        slot_length = self.settings.CONSENSUS_CYCLE_SLOT_LENGTH or 1  # seconds per slot
        target_timestamp = target_slot * slot_length
        
        current_time = int(time.time())
        if current_time >= target_timestamp:
            logger.info(f"Đã đạt hoặc vượt qua timestamp đích. Tiếp tục.")
            return
            
        # Đợi cho đến khi đạt được timestamp/slot đích
        while True:
            current_slot = await self._get_current_slot()
            if current_slot is None:
                logger.warning(
                    "Không lấy được giá trị slot hiện tại, thử lại sau 5 giây..."
                )
                await asyncio.sleep(5)
                continue

            if current_slot >= target_slot:
                logger.info(
                    f"Đã đạt slot đích {target_slot} (Hiện tại: {current_slot}). Tiếp tục."
                )
                break

            wait_interval = self.settings.CONSENSUS_SLOT_QUERY_INTERVAL_SECONDS
            logger.debug(
                f"Slot hiện tại: {current_slot}, Đích: {target_slot}. Đợi {wait_interval:.1f}s..."
            )
            await asyncio.sleep(wait_interval)

    async def _send_task_via_network_async(self, miner_endpoint: str, task: TaskModel) -> bool:
        """Send task via network asynchronously with circuit breaker and rate limiting"""
        start_time = time.time()
        
        # Check rate limit
        if not await self.rate_limiter.acquire():
            logger.warning(f"Rate limit exceeded for sending task to {miner_endpoint}")
            return False
            
        try:
            # Use circuit breaker for network call
            result = await self.circuit_breaker.execute(
                self._send_task_implementation,
                miner_endpoint,
                task
            )
            
            self.metrics.record_task_send(True)
            self.metrics.record_network_latency('task_send', time.time() - start_time)
            return result
            
        except Exception as e:
            self.metrics.record_task_send(False)
            self.metrics.record_error('network')
            raise

    async def _send_task_implementation(self, miner_endpoint: str, task: TaskModel) -> bool:
        """Actual implementation of task sending"""
        if not miner_endpoint or not miner_endpoint.startswith(("http://", "https://")):
            logger.warning(
                f"Invalid or missing API endpoint for miner: {miner_endpoint} in task {getattr(task, 'task_id', 'N/A')}"
            )
            return False

        target_url = f"{miner_endpoint}/receive-task"
        timeout = self.settings.HTTP_CLIENT_TIMEOUT or 30.0

        task_payload = (
            task.model_dump(mode="json")
            if hasattr(task, "model_dump")
            else task.dict()
        )

        logger.debug(f"Sending task {task.task_id} to {target_url} with timeout {timeout}s")
        response = await self.http_client.post(
            target_url, 
            json=task_payload,
            timeout=timeout
        )

        response.raise_for_status()
        return True

    async def send_task_and_track(self, miners: List[MinerInfo]):
        """
        Creates and sends tasks asynchronously to the selected list of miners,
        tracking the successfully sent tasks.

        Args:
            miners (List[MinerInfo]): The list of miners selected for tasking.

        Raises:
            NotImplementedError: If `_create_task_data` is not implemented.
            Exception: For unexpected errors during task creation or sending.
        """
        if not miners:
            logger.warning("send_task_and_track called with empty miner list.")
            self.tasks_sent = {}
            return

        logger.info(
            f"[V:{self.info.uid}] Attempting to send tasks to {len(miners)} selected miners..."
        )
        self.tasks_sent = {}  # Xóa danh sách task đã gửi của chu kỳ trước
        tasks_to_send = []
        # Tạm lưu assignment để chỉ thêm vào self.tasks_sent nếu gửi thành công
        task_assignments: Dict[str, TaskAssignment] = {}  # {miner_uid: TaskAssignment}

        for miner in miners:
            # Kiểm tra xem miner có endpoint hợp lệ không
            if not miner.api_endpoint or not miner.api_endpoint.startswith(
                ("http://", "https://")
            ):
                logger.warning(
                    f"Miner {miner.uid} has invalid or missing API endpoint ('{miner.api_endpoint}'). Skipping task assignment."
                )
                continue

            if miner.uid == self.info.uid:
                logger.debug(f"Skipping sending task to self (UID: {miner.uid}).")
                continue

            task_id = f"task_{self.current_cycle}_{self.info.uid}_{miner.uid}_{random.randint(1000,9999)}"
            try:
                task_data = self._create_task_data(miner.uid)
                # Giả sử TaskModel có thể tạo từ dict hoặc có constructor phù hợp
                # Cần đảm bảo TaskModel được import đúng
                task = TaskModel(task_id=task_id, **task_data)
            except Exception as e:
                logger.exception(f"Failed to create task for miner {miner.uid}: {e}")
                continue  # Bỏ qua miner này nếu không tạo được task

            # Tạo đối tượng TaskAssignment trước khi gửi
            assignment = TaskAssignment(
                task_id=task_id,
                task_data=task_data,
                miner_uid=miner.uid,  # Lưu UID dạng hex string
                validator_uid=self.info.uid,  # Lưu UID dạng hex string
                timestamp_sent=time.time(),
                expected_result_format={"output": "tensor", "loss": "float"},  # Ví dụ
            )
            task_assignments[miner.uid] = assignment  # Lưu tạm
            self.tasks_sent[task_id] = assignment
            logger.debug(
                f"Added task {task_id} to self.tasks_sent for miner {miner.uid}"
            )
            # Tạo coroutine để gửi task và thêm vào danh sách chờ
            tasks_to_send.append(
                self._send_task_via_network_async(miner.api_endpoint, task)
            )

        # --- Phần await asyncio.gather và xử lý results ---
        if not tasks_to_send:
            logger.warning("No valid tasks could be prepared for sending.")
            # Cập nhật lại tasks_sent nếu không có task nào được gửi đi? Có thể không cần.
            return  # Thoát nếu không có task nào để gửi

        logger.info(f"Sending {len(tasks_to_send)} tasks concurrently...")
        # Gửi đồng thời tất cả các task
        results = await asyncio.gather(*tasks_to_send, return_exceptions=True)

        successful_sends = 0
        # Xử lý kết quả gửi task
        # Lấy danh sách miners tương ứng với results (những miner thực sự được gửi task)
        miners_with_tasks = [m for m in miners if m.uid in task_assignments]
        for i, result in enumerate(results):
            # Lấy miner tương ứng với kết quả này
            if i < len(miners_with_tasks):
                miner = miners_with_tasks[i]
                assignment = task_assignments.get(miner.uid)
                assignment_check = self.tasks_sent.get(
                    f"task_{self.current_cycle}_{self.info.uid}_{miner.uid}"
                )  # Tìm lại task_id theo cấu trúc
                # Tìm task_id tương ứng với miner trong lần gửi này
                for tid, assign in self.tasks_sent.items():
                    # Kiểm tra cycle và miner uid để đảm bảo đúng task
                    if (
                        tid.startswith(f"task_{self.current_cycle}_")
                        and assign.miner_uid == miner.uid
                    ):
                        current_task_id = tid
                        break

                if current_task_id and isinstance(result, bool) and result:
                    # Gửi thành công, cập nhật last_selected_time
                    if miner.uid in self.miners_info:
                        self.miners_info[miner.uid].last_selected_time = (
                            self.current_cycle
                        )
                        logger.debug(
                            f"Updated last_selected_time for miner {miner.uid} to cycle {self.current_cycle}"
                        )
                    successful_sends += 1
                elif current_task_id:
                    # Gửi thất bại, xóa task khỏi self.tasks_sent? Hoặc đánh dấu là thất bại?
                    # Tạm thời chỉ log lỗi
                    logger.warning(
                        f"Failed to send task {current_task_id} to Miner {miner.uid}. Error/Result: {result}"
                    )
                    # Cân nhắc xóa khỏi tasks_sent để tránh validator chờ kết quả không bao giờ đến
                    # del self.tasks_sent[current_task_id]
                else:
                    logger.error(
                        f"Could not map result index {i} back to a sent task for miner {miner.uid}"
                    )

            else:
                logger.error(
                    f"Result index {i} out of bounds for miners_with_tasks list during task sending result processing."
                )

        logger.info(
            f"Finished sending tasks attempt. Successful sends: {successful_sends}/{len(tasks_to_send)}. Tasks currently tracked: {len(self.tasks_sent)}"
        )

    # --- Nhận và Chấm điểm Kết quả ---
    # --- 2. Sửa receive_results và bỏ _listen_for_results_async ---
    async def _listen_for_results_async(self, timeout: float):
        # <<<--- Bỏ hoàn toàn logic mock này ---<<<
        # Thay vào đó, hàm receive_results sẽ chỉ đơn giản là chờ một khoảng thời gian
        # trong khi kết quả được thêm vào self.results_received thông qua API endpoint.
        pass  # Removed method body as it was mock logic

    async def receive_results(self, timeout: Optional[float] = None):
        """
        Waits for miner results to arrive via the API endpoint.

        This method simply waits for the specified timeout duration.
        Actual results are received asynchronously by the API endpoint handler
        (`/v1/miner/submit_result`) which calls `add_miner_result` to buffer them.

        Args:
            timeout (Optional[float]): Duration in seconds to wait. If None, a default
                                     based on settings is used.
        """
        if timeout is None:
            receive_timeout_default = (
                self.settings.CONSENSUS_SEND_SCORE_OFFSET_MINUTES * 60 * 0.5
            )  # Ví dụ
            timeout = receive_timeout_default

        logger.info(
            f"[V:{self.info.uid}] Waiting {timeout:.1f}s for miner results via API endpoint..."
        )

        # Đơn giản là đợi hết timeout. Kết quả sẽ được tích lũy trong self.results_received.
        await asyncio.sleep(timeout)

        # Không cần xóa self.results_received ở đây, vì nó được tích lũy qua API.
        # Có thể xóa ở đầu chu kỳ mới hoặc trước khi bắt đầu chờ.
        # => Nên xóa ở đầu hàm này để chỉ xử lý kết quả của chu kỳ hiện tại
        # Tuy nhiên, nếu API nhận kết quả chậm, có thể kết quả chu kỳ trước bị xử lý ở chu kỳ sau?
        # => Cần cơ chế quản lý kết quả theo chu kỳ trong add_miner_result.

        # Tạm thời: Giả định API đủ nhanh và chỉ xử lý kết quả đã nhận trong khoảng timeout
        async with self.results_buffer_lock:  # Lock khi đọc số lượng
            received_count = sum(
                len(res_list) for res_list in self.results_received.values()
            )
            task_ids_with_results = list(self.results_received.keys())

        logger.info(
            f"Finished waiting period. Total results accumulated: {received_count} for tasks: {task_ids_with_results}"
        )
        # Logic xử lý kết quả sẽ diễn ra ở bước score_miner_results

    # -----------------------------------------------------------

    # --- Cập nhật add_miner_result ---
    async def add_miner_result(self, result: MinerResult) -> bool:
        """
        (API Call Handler) Receives a miner result, performs basic validation,
        and stores it in the results buffer.

        Args:
            result (MinerResult): The result object submitted by the miner.

        Returns:
            bool: True if the result was valid and buffered, False otherwise.
        """
        if not result or not result.task_id or not result.miner_uid:
            logger.warning("API: Received invalid miner result (missing fields).")
            return False

        # Kiểm tra task ID có đang được mong đợi không (có trong tasks_sent)
        # Dùng get() để tránh lỗi nếu task_id không có (ví dụ đến quá muộn)
        assignment = self.tasks_sent.get(result.task_id)
        if not assignment:
            logger.warning(
                f"API: Received result for unknown/already processed/timed out task_id: {result.task_id} from miner {result.miner_uid}. Ignoring."
            )
            return False  # Từ chối nếu task ID không hợp lệ hoặc đã xử lý xong/timeout

        # Kiểm tra miner gửi có đúng không
        if assignment.miner_uid != result.miner_uid:
            logger.warning(
                f"API: Received result for task {result.task_id} from wrong miner {result.miner_uid}. Expected {assignment.miner_uid}. Ignoring."
            )
            return False

        # --- Lưu vào buffer ---
        async with self.results_buffer_lock:
            if result.task_id in self.results_buffer:
                logger.warning(
                    f"API: Overwriting previous buffered result for task {result.task_id}."
                )
            self.results_buffer[result.task_id] = result
            logger.info(
                f"API: Received and buffered result for task {result.task_id} from miner {result.miner_uid}."
            )
        # --------------------
        return True  # Báo thành công cho API

    # -----------------------------------------

    def score_miner_results(self):
        """Scores the miner results received during the waiting period."""
        # --- Xóa điểm cũ trước khi chấm ---
        self.validator_scores = {}
        # ---------------------------------

        # --- Lock khi đọc self.results_received ---
        # Tạo bản copy để tránh giữ lock quá lâu nếu scoring chậm
        results_to_score = {}
        # Dùng asyncio.run_coroutine_threadsafe nếu gọi từ thread khác?
        # Hoặc đảm bảo score_miner_results chạy trong cùng event loop
        # Giả sử chạy trong cùng event loop, chỉ cần lock async
        # async with self.results_received_lock: # Không cần lock nếu chỉ đọc sau khi wait xong
        #     results_to_score = self.results_received.copy()
        # => Không cần lock nếu receive_results đợi xong mới gọi score

        # Lấy bản copy để xử lý
        results_to_score = self.results_received.copy()
        # Reset lại dict nhận kết quả cho chu kỳ sau
        self.results_received = defaultdict(list)

        # Gọi hàm logic từ scoring.py (truyền bản copy)
        self.validator_scores = score_results_logic(
            results_received=results_to_score,  # <<<--- Dùng bản copy
            tasks_sent=self.tasks_sent,
            validator_uid=self.info.uid,
        )
        # Hàm score_results_logic sẽ gọi _calculate_score_from_result (cần override)

    async def add_received_score(
        self, submitter_uid: str, cycle: int, scores: List[ValidatorScore]
    ):
        """Adds scores received from another validator to memory (async safe)."""
        # Logic này quản lý state nội bộ nên giữ lại trong Node
        # TODO: Thêm validation cho scores và submitter_uid
        async with self.received_scores_lock:
            if cycle not in self.received_validator_scores:
                # Chỉ lưu điểm cho chu kỳ hiện tại hoặc tương lai gần? Tránh lưu trữ quá nhiều.
                if cycle < self.current_cycle - 1:  # Ví dụ: chỉ giữ lại chu kỳ trước đó
                    logger.warning(
                        f"Received scores for outdated cycle {cycle} from {submitter_uid}. Ignoring."
                    )
                    return
                self.received_validator_scores[cycle] = defaultdict(dict)

            valid_scores_added = 0
            for score in scores:
                if not (
                    isinstance(score, ValidatorScore)
                    and isinstance(score.score, (int, float))
                    and 0.0 <= score.score <= 1.0
                    and isinstance(score.task_id, str)
                    and score.task_id
                    and isinstance(score.miner_uid, str)
                    and score.miner_uid
                    and score.validator_uid == submitter_uid
                ):  # Đảm bảo validator_uid khớp người gửi
                    logger.warning(
                        f"Ignoring invalid score object received from {submitter_uid}: {score}"
                    )
                    continue

                if score.task_id not in self.received_validator_scores[cycle]:
                    self.received_validator_scores[cycle][score.task_id] = {}
                # Ghi đè điểm nếu validator gửi lại?
                self.received_validator_scores[cycle][score.task_id][
                    score.validator_uid
                ] = score
                valid_scores_added += 1
                # else:
                #     logger.debug(f"Ignoring score for irrelevant task {score.task_id} from {submitter_uid}")

            logger.debug(
                f"Added {valid_scores_added} scores from {submitter_uid} for cycle {cycle}"
            )

    async def broadcast_scores(
        self, scores_to_broadcast: Dict[str, List[ValidatorScore]]
    ):
        """Broadcasts accumulated local scores to other active validators."""
        # Kiểm tra trạng thái validator (giữ nguyên)
        if self.info.status != STATUS_ACTIVE:
            logger.info(
                f"[V:{self.info.uid}] Skipping score broadcast (status: {self.info.status})."
            )
            return

        # Tính tổng số điểm sẽ gửi
        total_scores = sum(len(v) for v in scores_to_broadcast.values())
        logger.info(
            f"[V:{self.info.uid}] Broadcasting {total_scores} accumulated local scores for cycle {self.current_cycle}..."
        )

        # --- Chuẩn bị danh sách điểm phẳng để gửi ---
        # Hàm broadcast_scores_logic mong đợi một List[ValidatorScore]
        flat_scores_list: List[ValidatorScore] = []
        for task_scores_list in scores_to_broadcast.values():
            flat_scores_list.extend(task_scores_list)
        # ------------------------------------------

        if not flat_scores_list:
            logger.info("No accumulated local scores to broadcast for this cycle.")
            return

        # Gọi hàm logic P2P với danh sách điểm đã làm phẳng
        try:
            await broadcast_scores_logic(
                validator_node=self,  # Truyền self để lấy config, keys...
                cycle_scores_dict=self.cycle_scores,  # <<< Pass the original dict with the correct parameter name
            )
        except Exception as e:
            logger.exception(f"Error during broadcast_scores_logic: {e}")

    async def _get_active_validators(self) -> List[ValidatorInfo]:
        """Gets the list of currently active validators."""
        # TODO: Implement actual metagraph query or use a reliable cache.
        logger.debug("Getting active validators...")
        # Tạm thời lọc từ danh sách đã load, cần đảm bảo danh sách này được cập nhật thường xuyên
        active_vals = [
            v
            for v in self.validators_info.values()
            if v.api_endpoint and getattr(v, "status", STATUS_ACTIVE) == STATUS_ACTIVE
        ]
        logger.debug(f"Found {len(active_vals)} active validators with API endpoints.")
        return active_vals

    def _has_sufficient_scores(
        self, task_id: str, total_active_validators: int
    ) -> bool:
        """
        Checks if enough scores have been received for a specific task
        to proceed with consensus calculation.

        Args:
            task_id (str): The task ID to check scores for.
            total_active_validators (int): The total number of active validators.

        Returns:
            bool: True if sufficient scores are present, False otherwise.
        """
        # Logic này quản lý state nội bộ nên giữ lại trong Node
        current_cycle_scores = self.received_validator_scores.get(
            self.current_cycle, {}
        )
        task_scores = current_cycle_scores.get(task_id, {})
        received_validators_for_task = set(
            task_scores.keys()
        )  # Dùng set để tránh đếm trùng

        # Đếm cả điểm của chính mình (nếu đã chấm)
        # Kiểm tra xem điểm của chính mình đã có trong validator_scores chưa và validator_uid khớp không
        if task_id in self.validator_scores:
            # Kiểm tra xem có score nào trong list của task_id này là của mình không
            if any(
                s.validator_uid == self.info.uid for s in self.validator_scores[task_id]
            ):
                received_validators_for_task.add(
                    self.info.uid
                )  # Thêm UID của mình vào set

        received_count = len(received_validators_for_task)

        # Tính số lượng cần thiết
        min_validators = self.settings.CONSENSUS_MIN_VALIDATORS_FOR_CONSENSUS
        # Lấy tỉ lệ phần trăm yêu cầu từ settings (thêm nếu chưa có)
        # required_percentage = self.settings.get('CONSENSUS_REQUIRED_PERCENTAGE', 0.6) # Ví dụ
        required_percentage = 0.6  # Giả định 60%

        # Yêu cầu số lượng tối thiểu HOẶC phần trăm nhất định
        required_count_by_percentage = math.ceil(
            total_active_validators * required_percentage
        )
        required_count = max(min_validators, required_count_by_percentage)

        # Đảm bảo required_count không lớn hơn tổng số validator hoạt động
        required_count = min(required_count, total_active_validators)

        logger.debug(
            f"Scores check for task {task_id}: Received from {received_count}/{required_count} validators (Total active: {total_active_validators}, Min: {min_validators}, %: {required_percentage*100:.0f})"
        )
        return received_count >= required_count

    async def wait_for_consensus_scores(self, wait_timeout_seconds: float) -> bool:
        """
        Waits for a limited time to receive sufficient scores from peer validators.

        Args:
            wait_timeout_seconds (float): Maximum time to wait in seconds.

        Returns:
            bool: True if sufficient scores were received for all locally scored tasks
                  within the timeout, False otherwise.
        """
        logger.info(
            f"Waiting up to {wait_timeout_seconds:.1f}s for consensus scores for cycle {self.current_cycle}..."
        )
        start_wait = time.time()
        active_validators = await self._get_active_validators()
        total_active = len(active_validators)
        min_consensus_validators = self.settings.CONSENSUS_MIN_VALIDATORS_FOR_CONSENSUS

        if total_active == 0:
            logger.warning(
                "No active validators found. Skipping wait for consensus scores."
            )
            return False  # Không thể đồng thuận nếu không có ai hoạt động
        elif total_active < min_consensus_validators:
            logger.warning(
                f"Not enough active validators ({total_active}) for minimum consensus ({min_consensus_validators}). Proceeding with available data, but consensus might be weak."
            )
            # Vẫn trả về True để cho phép tính toán, nhưng log cảnh báo
            return True

        # Chỉ kiểm tra các task mà validator này đã chấm điểm (và có thể đã broadcast)
        tasks_to_check = set(self.validator_scores.keys())
        if not tasks_to_check:
            logger.info(
                "No local scores generated, skipping wait for consensus scores."
            )
            return True  # Không có gì để chờ

        logger.debug(f"Waiting for consensus on tasks: {list(tasks_to_check)}")
        processed_task_ids = set()  # Các task đã đủ điểm

        while time.time() - start_wait < wait_timeout_seconds:
            all_relevant_tasks_sufficient = True  # Kiểm tra cho các task cần check
            tasks_still_needing_check = tasks_to_check - processed_task_ids

            if not tasks_still_needing_check:
                logger.info("Sufficient scores received for all relevant tasks.")
                return True  # Đã đủ hết

            async with self.received_scores_lock:  # Lock khi kiểm tra
                # Tạo copy của set để tránh lỗi thay đổi kích thước khi lặp
                for task_id in list(tasks_still_needing_check):
                    if self._has_sufficient_scores(task_id, total_active):
                        logger.debug(f"Task {task_id} now has sufficient scores.")
                        processed_task_ids.add(task_id)
                    else:
                        # Chỉ cần một task chưa đủ là chưa xong
                        all_relevant_tasks_sufficient = False
                        # Vẫn tiếp tục kiểm tra các task khác trong lần lặp này

    async def _get_current_block_timestamp(self) -> Optional[int]:
        """Get current block timestamp with circuit breaker protection"""
        try:
            return await self.circuit_breaker.execute(
                self._get_block_timestamp_implementation
            )
        except Exception as e:
            logger.error(f"Failed to get block timestamp: {e}")
            return None

    async def _get_block_timestamp_implementation(self) -> Optional[int]:
        """Actual implementation of getting block timestamp"""
        max_retries = self.settings.CONSENSUS_MAX_RETRIES or 3
        retry_delay = self.settings.CONSENSUS_RETRY_DELAY_SECONDS or 2

        for attempt in range(max_retries):
            try:
                logger.debug(f"Attempt {attempt + 1}/{max_retries} to fetch latest block timestamp")
                ledger_info = await self.client.get_ledger_information()
                
                if ledger_info and "timestamp" in ledger_info:
                    timestamp = int(ledger_info["timestamp"]) // 1000
                    logger.debug(f"Successfully fetched block timestamp: {timestamp}")
                    return timestamp
                    
                logger.warning(f"Attempt {attempt + 1}: Invalid ledger info format: {ledger_info}")
                
            except Exception as e:
                logger.warning(f"Attempt {attempt + 1}: Error fetching latest block info: {e}")
                
            if attempt < max_retries - 1:
                wait_time = retry_delay * (attempt + 1)
                logger.info(f"Retrying in {wait_time} seconds...")
                await asyncio.sleep(wait_time)
                
        logger.error(f"Failed to fetch block timestamp after {max_retries} attempts")
        return None

    async def _process_task(self, task):
        """Process received task with improved error handling and metrics"""
        start_time = time.time()
        try:
            # Validate task
            if not self._validate_task(task):
                raise ValueError("Invalid task format")
                
            # Process task with timeout
            async with asyncio.timeout(self.settings.TASK_PROCESSING_TIMEOUT):
                result = await self._process_task_logic(task)
                
            # Record metrics
            processing_time = time.time() - start_time
            self.metrics.record_task_processing('validation', processing_time)
            
            return result
            
        except asyncio.TimeoutError:
            self.metrics.record_error('timeout')
            raise
        except Exception as e:
            self.metrics.record_error('validation')
            raise

    def _validate_task(self, task) -> bool:
        """Validate task format and content"""
        try:
            if not hasattr(task, 'task_id') or not task.task_id:
                return False
                
            if not hasattr(task, 'task_data') or not task.task_data:
                return False
                
            # Add more validation as needed
            return True
            
        except Exception:
            return False

    async def _receive_task_via_network_async(self):
        """Receive task via network asynchronously"""
        start_time = time.time()
        try:
            # Get task from network
            task = await self._get_task_from_network()  # Implement this method based on your network protocol
            self.metrics.record_task_receive('success')
            self.metrics.record_network_latency('task_receive', time.time() - start_time)
            return task
        except TimeoutError:
            self.metrics.record_task_receive('timeout')
            self.metrics.record_error('network')
            raise
        except Exception as e:
            self.metrics.record_task_receive('invalid')
            self.metrics.record_error('network')
            raise

    async def _update_metagraph(self):
        """Update metagraph data"""
        try:
            # Load latest metagraph data
            await self.load_metagraph_data()
            
            # Update active nodes count
            active_miners = len([m for m in self.miners_info.values() if m.status == STATUS_ACTIVE])
            active_validators = len([v for v in self.validators_info.values() if v.status == STATUS_ACTIVE])
            
            self.metrics.update_active_nodes(active_miners, active_validators)
            logger.info(f"Updated metagraph: {active_miners} active miners, {active_validators} active validators")
        except Exception as e:
            self.metrics.record_error('consensus')
            raise

    async def start_health_server(self):
        """Start the health check server"""
        config = uvicorn.Config(
            health_app,
            host="0.0.0.0",
            port=8000,
            log_level="info"
        )
        self.health_server = uvicorn.Server(config)
        await self.health_server.serve()

    async def run(self):
        """Run the validator node with improved error handling"""
        # Start health check server
        await self.start_health_server()
        
        while True:
            self.metrics.start_consensus_cycle()
            try:
                # Update last cycle time
                self.last_cycle_time = time.time()
                
                # Update metagraph data with circuit breaker
                await self.circuit_breaker.execute(self._update_metagraph)
                
                # Get current slot with circuit breaker
                current_slot = await self._get_current_block_timestamp()
                if current_slot is None:
                    logger.error("Failed to get current slot, retrying in 5 seconds...")
                    await asyncio.sleep(5)
                    continue
                
                # Select miners for this cycle
                selected_miners = self.select_miners()
                if not selected_miners:
                    logger.warning("No miners selected for this cycle")
                    await asyncio.sleep(self.settings.CONSENSUS_CYCLE_SLOT_LENGTH)
                    continue
                
                # Send tasks to selected miners with rate limiting
                await self.send_task_and_track(selected_miners)
                
                # Wait for results
                await self.receive_results()
                
                # Score results
                self.score_miner_results()
                
                # Broadcast scores with rate limiting
                await self.broadcast_scores(self.validator_scores)
                
                # Wait for consensus
                await self.wait_for_consensus_scores(self.settings.CONSENSUS_SCORE_WAIT_TIMEOUT)
                
                # Update memory usage
                self.metrics.update_memory_usage(psutil.Process().memory_info().rss)
                
                # Wait for next cycle
                await asyncio.sleep(self.settings.CONSENSUS_CYCLE_SLOT_LENGTH)
                
            except Exception as e:
                self.metrics.record_error('consensus')
                logger.error(f"Error in consensus cycle: {e}")
                await asyncio.sleep(5)  # Wait before retrying
            finally:
                self.metrics.end_consensus_cycle()

async def run_validator_node():
    node = None
    try:
        # --- Validate Settings ---
        logger.info("Setting up Validator Node with Aptos...")
        if not settings:
            logger.error("Critical error: Settings not loaded. Please check your configuration file.")
            return

        # Validate required settings
        required_settings = {
            'APTOS_NODE_URL': settings.APTOS_NODE_URL,
            'APTOS_PRIVATE_KEY': settings.APTOS_PRIVATE_KEY,
            'APTOS_CONTRACT_ADDRESS': settings.APTOS_CONTRACT_ADDRESS,
            'VALIDATOR_API_ENDPOINT': settings.VALIDATOR_API_ENDPOINT,
            'HTTP_CLIENT_TIMEOUT': settings.HTTP_CLIENT_TIMEOUT,
            'HTTP_CLIENT_MAX_CONNECTIONS': settings.HTTP_CLIENT_MAX_CONNECTIONS,
            'CONSENSUS_CYCLE_SLOT_LENGTH': settings.CONSENSUS_CYCLE_SLOT_LENGTH,
            'CONSENSUS_MAX_RETRIES': settings.CONSENSUS_MAX_RETRIES,
            'CONSENSUS_RETRY_DELAY_SECONDS': settings.CONSENSUS_RETRY_DELAY_SECONDS
        }

        missing_settings = [key for key, value in required_settings.items() if value is None]
        if missing_settings:
            logger.error(f"Missing required settings: {', '.join(missing_settings)}")
            return

        # Validate settings format
        if not settings.APTOS_NODE_URL.startswith(('http://', 'https://')):
            logger.error("APTOS_NODE_URL must be a valid HTTP(S) URL")
            return

        if not settings.APTOS_CONTRACT_ADDRESS.startswith('0x'):
            logger.error("APTOS_CONTRACT_ADDRESS must start with '0x'")
            return

        if not settings.VALIDATOR_API_ENDPOINT.startswith(('http://', 'https://')):
            logger.error("VALIDATOR_API_ENDPOINT must be a valid HTTP(S) URL")
            return

        # Validate numeric settings
        try:
            if settings.HTTP_CLIENT_TIMEOUT <= 0:
                raise ValueError("HTTP_CLIENT_TIMEOUT must be positive")
            if settings.HTTP_CLIENT_MAX_CONNECTIONS <= 0:
                raise ValueError("HTTP_CLIENT_MAX_CONNECTIONS must be positive")
            if settings.CONSENSUS_CYCLE_SLOT_LENGTH <= 0:
                raise ValueError("CONSENSUS_CYCLE_SLOT_LENGTH must be positive")
            if settings.CONSENSUS_MAX_RETRIES <= 0:
                raise ValueError("CONSENSUS_MAX_RETRIES must be positive")
            if settings.CONSENSUS_RETRY_DELAY_SECONDS <= 0:
                raise ValueError("CONSENSUS_RETRY_DELAY_SECONDS must be positive")
        except ValueError as e:
            logger.error(f"Invalid numeric setting: {str(e)}")
            return

        # --- Initialize Aptos context ---
        from mt_aptos.aptos_core.context import get_aptos_context
        
        try:
            # Get Aptos context (client, account)
            contract_client, aptos_client, account = await get_aptos_context(
                node_url=settings.APTOS_NODE_URL,
                private_key=settings.APTOS_PRIVATE_KEY,
                contract_address=settings.APTOS_CONTRACT_ADDRESS
            )
            
            if not contract_client or not aptos_client or not account:
                logger.error("Failed to initialize Aptos context: One or more required components are missing")
                return
                
            logger.info(f"Successfully connected to Aptos network with address: {account.address()}")
            
            # --- Get validator info ---
            from mt_aptos.aptos_core.validator_helper import get_validator_info
            
            # Using account address as validator address
            validator_address = account.address().hex()
            if not validator_address.startswith("0x"):
                validator_address = f"0x{validator_address}"
            
            validator_data = await get_validator_info(
                aptos_client, 
                settings.APTOS_CONTRACT_ADDRESS, 
                validator_address
            )
            
            if not validator_data:
                logger.error(
                    f"Failed to retrieve validator data for address {validator_address}. "
                    "Please ensure you are registered as a validator on the network."
                )
                return
                
            # Construct ValidatorInfo object
            from mt_aptos.core.datatypes import ValidatorInfo
            
            validator_info = ValidatorInfo(
                uid=validator_data.get("uid", ""),
                address=validator_address,
                api_endpoint=settings.VALIDATOR_API_ENDPOINT
            )
            
            if not validator_info.uid:
                logger.error("Invalid validator UID received from network. Please check your registration status.")
                return
                
            logger.info(f"Successfully loaded validator info: UID={validator_info.uid}, Address={validator_address}")
            
            # --- Initialize validator node ---
            node = ValidatorNode(
                validator_info=validator_info,
                aptos_client=aptos_client,
                account=account,
                contract_address=settings.APTOS_CONTRACT_ADDRESS
            )
            
            # --- Run main loop ---
            logger.info("Starting validator node main loop...")
            await node.run()
            
        except Exception as e:
            logger.error(
                f"Error during Aptos context setup: {str(e)}\n"
                "Please check your network connection and configuration settings."
            )
            return
    except Exception as e:
        logger.error(
            f"Critical error during validator node setup: {str(e)}\n"
            "Please check the logs for more details and ensure all required components are properly configured."
        )
    finally:
        # Clean up resources if necessary
        if node and hasattr(node, "http_client"):
            try:
                await node.http_client.aclose()
                logger.info("Successfully closed HTTP client connection")
            except Exception as e:
                logger.error(f"Error closing HTTP client: {str(e)}")
        logger.info("Validator node shutdown complete.")
